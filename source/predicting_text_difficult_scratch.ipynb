{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d08ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewzimolzak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ddb108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8173d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WikiLarge_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2a071b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>252983</th>\n",
       "      <td>bar :1981 at :29985 fontsize : XS text : 29985...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71873</th>\n",
       "      <td>They subsequently studied with the Alban Berg ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373203</th>\n",
       "      <td>Construction of the World Trade Center involve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Aubigny is a commune in the Calvados departmen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124976</th>\n",
       "      <td>Carcans is a commune in the Gironde department...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349395</th>\n",
       "      <td>Users assign themselves a user name , log into...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>1965 &amp; ndash ; Vietnam War : Just miles from D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257252</th>\n",
       "      <td>Candice Michelle Beckman-Ehrlich -LRB- born Se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335170</th>\n",
       "      <td>Noircourt is a commune . It is found in the re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390171</th>\n",
       "      <td>It is also known as La Charte . This was the s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340844</th>\n",
       "      <td>In the Congress of Vienna the Jura was given t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370852</th>\n",
       "      <td>A small area of convection 600 miles southeast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37505</th>\n",
       "      <td>Yasuda was previously in the bands Blonde Redh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331178</th>\n",
       "      <td>George Clymer Elementary School , School Distr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91770</th>\n",
       "      <td>Giordano Bruno , born Filippo Bruno -LRB- 1548...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label\n",
       "252983  bar :1981 at :29985 fontsize : XS text : 29985...      0\n",
       "71873   They subsequently studied with the Alban Berg ...      1\n",
       "373203  Construction of the World Trade Center involve...      0\n",
       "27659   Aubigny is a commune in the Calvados departmen...      1\n",
       "124976  Carcans is a commune in the Gironde department...      1\n",
       "349395  Users assign themselves a user name , log into...      0\n",
       "3628    1965 & ndash ; Vietnam War : Just miles from D...      1\n",
       "257252  Candice Michelle Beckman-Ehrlich -LRB- born Se...      0\n",
       "335170  Noircourt is a commune . It is found in the re...      0\n",
       "390171  It is also known as La Charte . This was the s...      0\n",
       "340844  In the Congress of Vienna the Jura was given t...      0\n",
       "370852  A small area of convection 600 miles southeast...      0\n",
       "37505   Yasuda was previously in the bands Blonde Redh...      1\n",
       "331178  George Clymer Elementary School , School Distr...      0\n",
       "91770   Giordano Bruno , born Filippo Bruno -LRB- 1548...      1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a279552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208384\n",
       "1    208384\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d86c4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(df[[\"original_text\"]],df[\"label\"], \\\n",
    "                                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496579bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296266    0\n",
       "354373    0\n",
       "156421    1\n",
       "366119    0\n",
       "193549    1\n",
       "         ..\n",
       "359783    0\n",
       "358083    0\n",
       "152315    1\n",
       "117952    1\n",
       "305711    0\n",
       "Name: label, Length: 312576, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ec9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de8ef3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104192"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-atlanta",
   "metadata": {},
   "source": [
    "# Word characters, removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fa968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english', token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "count_vect.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c1b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_X)\n",
    "xtest_count =  count_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6447a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "tfidf_vect.fit(train_X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "758aa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \\\n",
    "                                   ngram_range=(2,3), stop_words=\"english\")\n",
    "tfidf_vect_ngram.fit(train_X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2232e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:502: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \\\n",
    "                                         ngram_range=(2,3), stop_words=\"english\")\n",
    "tfidf_vect_ngram_chars.fit(train_X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0cffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_nb = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5507fecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_nb.fit(xtrain_count,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825bcc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_1 = multi_nb.predict(xtest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "220fe72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5757159858722358"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(prediction_1, test_y)\n",
    "\n",
    "#0.5757159858722358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad0536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5105574324324325"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "metrics.accuracy_score(prediction_2, test_y)\n",
    "\n",
    "#0.5789216062653563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d05ad19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6310561271498771"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "metrics.accuracy_score(prediction_3, test_y)\n",
    "\n",
    "#0.6339066339066339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fantastic-fabric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769828777641277"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "metrics.accuracy_score(prediction_4, test_y)\n",
    "\n",
    "#0.5122274262899262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9b917a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "auburn-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "allied-pastor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6581503378378378\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6581503378378378\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "automatic-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6628339987714987\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6734490171990172\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "prescribed-hardware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5452721898034398\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.5879434121621622\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "representative-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706512976044226\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.7056395884520884\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expected-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interstate-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "civic-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214296683046683\n",
      "0.6262189035626535\n",
      "0.5389281326781327\n",
      "0.6858204084766585\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))\n",
    "\n",
    "#0.6214296683046683\n",
    "#0.6199996160933661\n",
    "#0.5333998771498771\n",
    "#0.6966657708845209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-respondent",
   "metadata": {},
   "source": [
    "# White space, removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "frequent-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [''] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:502: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*', stop_words=\"english\")\n",
    "count_vect.fit(train_X)\n",
    "xtrain_count =  count_vect.transform(train_X)\n",
    "xtest_count =  count_vect.transform(test_X)\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', stop_words=\"english\")\n",
    "tfidf_vect.fit(train_X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X)\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3), stop_words=\"english\")\n",
    "tfidf_vect_ngram.fit(train_X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3), stop_words=\"english\")\n",
    "tfidf_vect_ngram_chars.fit(train_X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dramatic-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594700168918919\n",
      "0.620690648034398\n",
      "0.6310561271498771\n",
      "0.5915809275184275\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))\n",
    "\n",
    "#0.594700168918919\n",
    "#0.6422182125307125\n",
    "#0.6339066339066339\n",
    "#0.4944045608108108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "injured-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6806568642506142\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.6860315571253072\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7055628071253072\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.706512976044226\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6806568642506142\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6882678132678133\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6896786701474201\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.7056395884520884\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "outstanding-maria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6443680896805897\n",
      "0.6555493703931204\n",
      "0.6688421375921376\n",
      "0.6858204084766585\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))\n",
    "\n",
    "#0.6214296683046683\n",
    "#0.6199996160933661\n",
    "#0.5333998771498771\n",
    "#0.6966657708845209\n",
    "\n",
    "#0.6443680896805897\n",
    "#0.6655885288697788\n",
    "#0.6625172757985258\n",
    "#0.6966657708845209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-revelation",
   "metadata": {},
   "source": [
    "# Word characters, keep stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "responsible-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_X)\n",
    "xtrain_count =  count_vect.transform(train_X)\n",
    "xtest_count =  count_vect.transform(test_X)\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(train_X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X)\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "august-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5947961455773956\n",
      "0.5754376535626535\n",
      "0.6310561271498771\n",
      "0.5950744778869779\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))\n",
    "\n",
    "#0.5947961455773956\n",
    "#0.6427364864864865\n",
    "#0.6339066339066339\n",
    "#0.5069199170761671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "straight-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6740344748157249\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.6853213298525799\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.6590525184275184\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.706512976044226\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6740344748157249\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6916173986486487\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6605401566339066\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.7056395884520884\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "outstanding-stadium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6400011517199017\n",
      "0.6480439957002457\n",
      "0.6019464066339066\n",
      "0.6858204084766585\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))\n",
    "\n",
    "#0.6400011517199017\n",
    "#0.655731726044226\n",
    "#0.601504914004914\n",
    "#0.6966657708845209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-recall",
   "metadata": {},
   "source": [
    "# White space, keep stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "centered-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "count_vect.fit(train_X)\n",
    "xtrain_count =  count_vect.transform(train_X)\n",
    "xtest_count =  count_vect.transform(test_X)\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "tfidf_vect.fit(train_X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X)\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "manual-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6096149416461917\n",
      "0.6354710534398035\n",
      "0.6310561271498771\n",
      "0.6089623003685504\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))\n",
    "\n",
    "#0.6096149416461917\n",
    "#0.6602042383292384\n",
    "#0.6339066339066339\n",
    "#0.4916884213759214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "inappropriate-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6935465294840295\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7027986793611793\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.718318105036855\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.706512976044226\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6935465294840295\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.7001497235872236\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.7017813267813268\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.7056395884520884\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "manual-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505681818181818\n",
      "0.657228961916462\n",
      "0.6626132524570024\n",
      "0.6858204084766585\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))\n",
    "\n",
    "#0.6505681818181818\n",
    "#0.6663275491400491\n",
    "#0.6749270577395577\n",
    "#0.6966657708845209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-entertainment",
   "metadata": {},
   "source": [
    "# Dale-Chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sized-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dale_chall.txt', 'r')\n",
    "dc_vocab = []\n",
    "for line in file:\n",
    "    dc_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "laden-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*', vocabulary = set(dc_vocab))\n",
    "count_vect.fit(train_X)\n",
    "xtrain_count =  count_vect.transform(train_X)\n",
    "xtest_count =  count_vect.transform(test_X)\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', vocabulary = set(dc_vocab))\n",
    "tfidf_vect.fit(train_X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X)\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3), vocabulary = set(dc_vocab))\n",
    "tfidf_vect_ngram.fit(train_X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3), vocabulary = set(dc_vocab))\n",
    "tfidf_vect_ngram_chars.fit(train_X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pointed-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6187615171990172\n",
      "0.4982532248157248\n",
      "0.5761382831695332\n",
      "0.6200476044226044\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))\n",
    "\n",
    "#0.6187615171990172\n",
    "#0.4982532248157248\n",
    "#0.5761382831695332\n",
    "#0.4978117321867322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "friendly-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6511920300982801\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.6513167997542998\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.4982532248157248\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.6352407094594594\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "#0.6511920300982801\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6513167997542998\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.4982532248157248\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248\n",
    "#0.6352407094594594\n",
    "#0.4988770730958231\n",
    "#0.4982532248157248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "unusual-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6377457002457002\n",
      "0.6379568488943489\n",
      "0.4982532248157248\n",
      "0.6438594133906634\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))\n",
    "\n",
    "#0.6377457002457002\n",
    "#0.6379568488943489\n",
    "#0.4982532248157248\n",
    "#0.6438594133906634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "continued-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "finished-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['dc_proportion'] = 0\n",
    "\n",
    "\n",
    "#for i in range(len(df)):\n",
    "    #split = df['original_text'][i].split()\n",
    "    #counter = Counter(split)\n",
    "\n",
    "    #counts = 0\n",
    "    #for word in counter.keys():\n",
    "        #if word in dc_vocab:\n",
    "            #counts += 1\n",
    "    #df['dc_proportion'].iloc[i] = counts/len(split) #0.3023255813953488\n",
    "    \n",
    "def dc_percentage(dc_counter, s):\n",
    "    split = s.split()\n",
    "    counter = Counter(split)\n",
    "    both = counter & dc_counter\n",
    "    counts = 0\n",
    "    for word in both.keys():\n",
    "        counts += counter[word]\n",
    "    return counts/len(split)\n",
    "\n",
    "dc_counter = Counter(dc_vocab)\n",
    "result = [dc_percentage(dc_counter, s) for s in df['original_text']]\n",
    "df['dc_proportion'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "white-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>dc_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>However , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>If it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>Calgary Stampeders ,</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label  \\\n",
       "0       There is manuscript evidence that Austen conti...      1   \n",
       "1       In a remarkable comparative analysis , Mandaea...      1   \n",
       "2       Before Persephone was released to Hermes , who...      1   \n",
       "3       Cogeneration plants are commonly found in dist...      1   \n",
       "4       Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1   \n",
       "...                                                   ...    ...   \n",
       "416763  A Duke Nukem 3D version has been sold for Xbox...      0   \n",
       "416764  However , it is becoming replaced as a method ...      0   \n",
       "416765  There are hand gestures in both Hindu and Budd...      0   \n",
       "416766  If it is necessary to use colors , try to choo...      0   \n",
       "416767                               Calgary Stampeders ,      0   \n",
       "\n",
       "        dc_proportion  \n",
       "0            0.465116  \n",
       "1            0.217391  \n",
       "2            0.543478  \n",
       "3            0.256410  \n",
       "4            0.388889  \n",
       "...               ...  \n",
       "416763       0.352941  \n",
       "416764       0.550000  \n",
       "416765       0.454545  \n",
       "416766       0.595238  \n",
       "416767       0.000000  \n",
       "\n",
       "[416768 rows x 3 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "organized-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = \\\n",
    "model_selection.train_test_split(df[[\"original_text\", 'dc_proportion']],df[\"label\"], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abstract-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "count_vect.fit(train_X['original_text'])\n",
    "xtrain_count =  count_vect.transform(train_X['original_text'])\n",
    "xtest_count =  count_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "tfidf_vect.fit(train_X['original_text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X['original_text'])\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X['original_text'])\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X['original_text'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X['original_text']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X['original_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "racial-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "compliant-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6113905098280098\n",
      "0.6361908783783784\n",
      "0.6317567567567568\n",
      "0.6094709766584766\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "overall-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6941703777641277\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7028850583538083\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7177230497542998\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7062922297297297\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "seven-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6532747235872236\n",
      "0.6657804821867321\n",
      "0.6645903716216216\n",
      "0.6865594287469288\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "czech-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "AoA = pd.read_csv('AoA_51715_words.csv', encoding = 'unicode_escape')\n",
    "AoA_vocab = AoA['Word'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "capable-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "AoA_counter = Counter(AoA_vocab)\n",
    "result = [dc_percentage(AoA_counter, s) for s in df['original_text']]\n",
    "df['AoA_proportion'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "charged-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>dc_proportion</th>\n",
       "      <th>AoA_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.651163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>However , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>If it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>Calgary Stampeders ,</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label  \\\n",
       "0       There is manuscript evidence that Austen conti...      1   \n",
       "1       In a remarkable comparative analysis , Mandaea...      1   \n",
       "2       Before Persephone was released to Hermes , who...      1   \n",
       "3       Cogeneration plants are commonly found in dist...      1   \n",
       "4       Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1   \n",
       "...                                                   ...    ...   \n",
       "416763  A Duke Nukem 3D version has been sold for Xbox...      0   \n",
       "416764  However , it is becoming replaced as a method ...      0   \n",
       "416765  There are hand gestures in both Hindu and Budd...      0   \n",
       "416766  If it is necessary to use colors , try to choo...      0   \n",
       "416767                               Calgary Stampeders ,      0   \n",
       "\n",
       "        dc_proportion  AoA_proportion  \n",
       "0            0.465116        0.651163  \n",
       "1            0.217391        0.521739  \n",
       "2            0.543478        0.782609  \n",
       "3            0.256410        0.692308  \n",
       "4            0.388889        0.416667  \n",
       "...               ...             ...  \n",
       "416763       0.352941        0.352941  \n",
       "416764       0.550000        0.750000  \n",
       "416765       0.454545        0.636364  \n",
       "416766       0.595238        0.809524  \n",
       "416767       0.000000        0.000000  \n",
       "\n",
       "[416768 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "least-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = \\\n",
    "model_selection.train_test_split( \\\n",
    "df[[\"original_text\", 'dc_proportion', 'AoA_proportion']],df[\"label\"], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "behavioral-visit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "count_vect.fit(train_X['original_text'])\n",
    "xtrain_count =  count_vect.transform(train_X['original_text'])\n",
    "xtest_count =  count_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "tfidf_vect.fit(train_X['original_text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X['original_text'])\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X['original_text'])\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X['original_text'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X['original_text']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X['original_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "elect-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fatty-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "random-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6142506142506142\n",
      "0.6361620853808354\n",
      "0.6332827856265356\n",
      "0.6094805743243243\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "controlled-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6944295147420148\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7038736179361179\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7194794226044227\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7070408476658476\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "macro-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6509041001228502\n",
      "0.6555109797297297\n",
      "0.6602618243243243\n",
      "0.6840928286240786\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "liable-password",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7346309377559378\n",
      "0.7998694717444718\n",
      "0.6546983773546273\n",
      "0.7424338400900901\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes - training data, check for overfitting\n",
    "prediction_1 = multi_nb.predict(xtrain_count)\n",
    "print(metrics.accuracy_score(prediction_1, train_y))\n",
    "\n",
    "prediction_2 = multi_nb2.predict(xtrain_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, train_y))\n",
    "\n",
    "prediction_3 = multi_nb3.predict(xtrain_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, train_y))\n",
    "\n",
    "prediction_4 = multi_nb4.predict(xtrain_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "varied-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464456644144144\n",
      "0.7486659244471745\n",
      "0.8076883701883701\n",
      "0.7247485411547911\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtrain_count)\n",
    "print(metrics.accuracy_score(lr_preds, train_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtrain_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, train_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtrain_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, train_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtrain_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "checked-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6573249385749386\n",
      "0.6623253224815725\n",
      "0.671196764946765\n",
      "0.7317356418918919\n"
     ]
    }
   ],
   "source": [
    "rf_count_preds = rf_clf_count.predict(xtrain_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtrain_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtrain_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtrain_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, train_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, train_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, train_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "increasing-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "tf_documents = count_vect.fit_transform(df['original_text'])\n",
    "tf_feature_names = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "helpful-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "behavioral-daughter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['s', 'united', 'university', 'states', 'state', 'school', 'government', 'new', 'president', 'college']\n",
      "topic 1: ['team', 'football', 'national', 'league', 'season', 'lrb', 'rrb', 'world', 'played', 'club']\n",
      "topic 2: ['lrb', 'rrb', 'used', 's', 'use', 'called', 'water', 'number', 'usually', 'term']\n",
      "topic 3: ['music', 'band', 'album', 'new', 'released', 's', 'rock', 'song', 'tropical', 'hurricane']\n",
      "topic 4: ['lrb', 'rrb', 'war', 's', 'world', 'years', 'species', 'people', 'century', 'family']\n",
      "topic 5: ['france', 'department', 'region', 'commune', 'calais', 'pas', 'north', 'la', 'saint', 'northern']\n",
      "topic 6: ['rrb', 'lrb', 'born', '1', 'football', 'player', 'american', '2', '4', 'january']\n",
      "topic 7: ['s', 'series', 'lrb', 'rrb', 'film', 'book', 'television', 'known', 'written', 'john']\n",
      "topic 8: ['lrb', 'rrb', 'district', 'o', 'province', 'located', 'municipality', 'town', 'language', 'city']\n",
      "topic 9: ['city', 'states', 'united', 'county', 's', 'river', 'game', 'people', 'north', 'largest']\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        term_list = [feature_names[i]\n",
    "                    for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\"topic %d:\" % (topic_idx), term_list)\n",
    "\n",
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "willing-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components = 10, random_state = 0, init = \"nndsvd\")\n",
    "W = nmf.fit_transform(tf_documents)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "clean-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['rrb', 'lrb', 'o', 'd', 'â', 'known', 'called', 'km', 'german', 'english']\n",
      "topic 1: ['s', 'u', 'album', 'women', 'state', 'band', 'second', 'film', 'children', 'death']\n",
      "topic 2: ['france', 'department', 'commune', 'region', 'la', 'calvados', 'normandie', 'basse', 'aisne', 'northern']\n",
      "topic 3: ['united', 'states', 'county', 'kingdom', 'president', 'iowa', 'state', 'canada', 'america', 'nations']\n",
      "topic 4: ['born', 'american', 'player', 'january', 'september', 'march', 'july', 'april', 'february', 'footballer']\n",
      "topic 5: ['1', '4', '2', 'ð', '3', 'â', 'î', '5', '0', 'ñ']\n",
      "topic 6: ['city', 'county', 'capital', 'state', 'north', 'located', 'district', 'largest', 'south', 'area']\n",
      "topic 7: ['pas', 'calais', 'nord', 'region', 'department', 'france', 'north', 'commune', 'l', 'ã']\n",
      "topic 8: ['new', 'world', 'known', 'york', 'used', 'war', 'time', 'called', 'south', 'best']\n",
      "topic 9: ['football', 'national', 'team', 'player', 'league', 'club', 'played', 'japanese', 'plays', 'hockey']\n"
     ]
    }
   ],
   "source": [
    "for topic_index in range(0, 10):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:10]:\n",
    "        top_terms.append(tf_feature_names[term_index])\n",
    "    print(\"topic %d:\" % (topic_index), top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "arctic-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['united', 's', 'states', 'school', 'state', 'president', 'university', 'government', 'college', 'war']\n",
      "topic 1: ['lrb', 'rrb', 'won', 'formula', 'world', 'championship', 'grand', 'race', 'season', 'car']\n",
      "topic 2: ['lrb', 'rrb', 'university', 's', 'used', 'energy', 'called', 'earth', 'known', 'light']\n",
      "topic 3: ['band', 'music', 'album', 'released', 'rock', 'song', 's', 'american', 'singer', 'lrb']\n",
      "topic 4: ['lrb', 'rrb', 'species', 's', 'family', 'long', 'war', '000', 'like', 'years']\n",
      "topic 5: ['france', 'department', 'commune', 'region', 'la', 'saint', 'northern', 'north', 'ã', 'aisne']\n",
      "topic 6: ['rrb', 'lrb', 'born', '1', '2', '4', '3', 'â', 'january', 'american']\n",
      "topic 7: ['s', 'series', 'film', 'lrb', 'rrb', 'book', 'television', 'match', 't', 'written']\n",
      "topic 8: ['lrb', 'rrb', 'district', 'province', 'germany', 'municipality', 'pakistan', 'capital', 'city', 'switzerland']\n",
      "topic 9: ['states', 'united', 'city', 'county', 'game', 's', 'state', 'video', 'florida', 'u']\n",
      "topic 10: ['calais', 'pas', 'north', 'nord', 'europe', 'america', 'asia', 'south', 'africa', 'european']\n",
      "topic 11: ['football', 'lrb', 'rrb', 'team', 'national', 'league', 'player', 'club', 'played', 'plays']\n",
      "topic 12: ['century', 'award', 'used', 'computer', 'awards', 'best', 'los', 'software', 's', 'websites']\n",
      "topic 13: ['s', 'years', 'year', 'later', 'hurricane', 'tropical', 'storm', 'day', 'life', 'million']\n",
      "topic 14: ['king', 'england', 'roman', 'church', 'ii', 'empire', 'town', 'emperor', 'century', 's']\n",
      "topic 15: ['world', 'new', 'york', 'war', 's', 'god', 'city', 'company', 'jesus', 'free']\n",
      "topic 16: ['lrb', 'rrb', 'o', 'language', 'word', 'english', 'wrestling', 'number', 'meaning', 'languages']\n",
      "topic 17: ['used', 'use', 'different', 'line', 'using', 'color', 'people', 'time', 'make', 'usually']\n",
      "topic 18: ['party', 's', 'people', 'references', 'known', 'political', 'rrb', 'lrb', 'minister', 'social']\n",
      "topic 19: ['south', 'located', 'london', 'east', 'north', 'west', 'area', 'river', 'island', 'city']\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20, random_state = 0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_\n",
    "\n",
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "designing-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['rrb', 'lrb', 'd', 'km', 'german', 'e', 'english', 'french', 'c', 'ndash']\n",
      "topic 1: ['s', 'u', 'women', 'state', 'children', 'st', 'death', 'father', 'book', 'king']\n",
      "topic 2: ['france', 'department', 'commune', 'region', 'la', 'calvados', 'normandie', 'basse', 'aisne', 'northern']\n",
      "topic 3: ['united', 'states', 'county', 'kingdom', 'president', 'iowa', 'state', 'canada', 'nations', 'government']\n",
      "topic 4: ['born', 'player', 'football', 'january', 'footballer', 'september', 'july', 'march', 'april', 'plays']\n",
      "topic 5: ['1', '2', '4', 'î', '3', '5', '0', 'ï', '6', '000']\n",
      "topic 6: ['city', 'county', 'capital', 'largest', 'state', 'located', 'population', 'area', 'iowa', 'seat']\n",
      "topic 7: ['pas', 'calais', 'nord', 'region', 'department', 'france', 'commune', 'north', 'l', 'ã']\n",
      "topic 8: ['called', 'people', 'usually', 'group', 'language', 'family', 'area', 'small', 'like', 'english']\n",
      "topic 9: ['football', 'national', 'team', 'league', 'player', 'club', 'played', 'plays', 'hockey', 'japanese']\n",
      "topic 10: ['new', 'york', 'zealand', 'jersey', 'university', 'australia', 'wales', 'state', 'island', 'south']\n",
      "topic 11: ['world', 'war', 'wrestling', 'championship', 'ii', 'wwe', 'cup', 'entertainment', 'professional', 'second']\n",
      "topic 12: ['ð', 'ñ', '4', '3', '2', 'russian', 'serbian', 'cyrillic', 'language', 'à']\n",
      "topic 13: ['â', 'à', 'c', '0', '3', '6', 'f', '10', '5', 'july']\n",
      "topic 14: ['known', 'best', 'better', 'commonly', 'english', 'series', 'species', 'television', 'role', 'simply']\n",
      "topic 15: ['o', 'o2004', 'sã', 'film', 'o2007', 'index', 'o2009', 'o2005', 'o2003', 'o2002']\n",
      "topic 16: ['used', 'term', 'use', 'word', 'english', 'people', 'language', 'usually', 'commonly', 'different']\n",
      "topic 17: ['year', 'time', 'released', 'album', 'game', 'music', 'years', 'people', 'band', 'second']\n",
      "topic 18: ['american', 'film', 'television', 'band', 'singer', 'actor', 'rock', 'series', 'league', 'actress']\n",
      "topic 19: ['north', 'south', 'west', 'east', 'district', 'located', 'river', 'town', 'province', 'america']\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components = 20, random_state = 0, init = \"nndsvd\")\n",
    "W = nmf.fit_transform(tf_documents)\n",
    "H = nmf.components_\n",
    "\n",
    "for topic_index in range(0, 20):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:10]:\n",
    "        top_terms.append(tf_feature_names[term_index])\n",
    "    print(\"topic %d:\" % (topic_index), top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "insured-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['s', 'united', 'university', 'states', 'state', 'school', 'government', 'new', 'president', 'college']\n",
      "topic 1: ['team', 'football', 'national', 'league', 'season', 'lrb', 'rrb', 'world', 'played', 'club']\n",
      "topic 2: ['lrb', 'rrb', 'used', 's', 'use', 'called', 'water', 'number', 'usually', 'term']\n",
      "topic 3: ['music', 'band', 'album', 'new', 'released', 's', 'rock', 'song', 'tropical', 'hurricane']\n",
      "topic 4: ['lrb', 'rrb', 'war', 's', 'world', 'years', 'species', 'people', 'century', 'family']\n",
      "topic 5: ['france', 'department', 'region', 'commune', 'calais', 'pas', 'north', 'la', 'saint', 'northern']\n",
      "topic 6: ['rrb', 'lrb', 'born', '1', 'football', 'player', 'american', '2', '4', 'january']\n",
      "topic 7: ['s', 'series', 'lrb', 'rrb', 'film', 'book', 'television', 'known', 'written', 'john']\n",
      "topic 8: ['lrb', 'rrb', 'district', 'o', 'province', 'located', 'municipality', 'town', 'language', 'city']\n",
      "topic 9: ['city', 'states', 'united', 'county', 's', 'river', 'game', 'people', 'north', 'largest']\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\", \\\n",
    "                            max_df = 0.9)\n",
    "tf_documents = count_vect.fit_transform(df['original_text'])\n",
    "tf_feature_names = count_vect.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_\n",
    "\n",
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "saved-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['rrb', 'lrb', 'o', 'd', 'â', 'known', 'called', 'km', 'german', 'english']\n",
      "topic 1: ['s', 'u', 'album', 'women', 'state', 'band', 'second', 'film', 'children', 'death']\n",
      "topic 2: ['france', 'department', 'commune', 'region', 'la', 'calvados', 'normandie', 'basse', 'aisne', 'northern']\n",
      "topic 3: ['united', 'states', 'county', 'kingdom', 'president', 'iowa', 'state', 'canada', 'america', 'nations']\n",
      "topic 4: ['born', 'american', 'player', 'january', 'september', 'march', 'july', 'april', 'february', 'footballer']\n",
      "topic 5: ['1', '4', '2', 'ð', '3', 'â', 'î', '5', '0', 'ñ']\n",
      "topic 6: ['city', 'county', 'capital', 'state', 'north', 'located', 'district', 'largest', 'south', 'area']\n",
      "topic 7: ['pas', 'calais', 'nord', 'region', 'department', 'france', 'north', 'commune', 'l', 'ã']\n",
      "topic 8: ['new', 'world', 'known', 'york', 'used', 'war', 'time', 'called', 'south', 'best']\n",
      "topic 9: ['football', 'national', 'team', 'player', 'league', 'club', 'played', 'japanese', 'plays', 'hockey']\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components = 10, random_state = 0, init = \"nndsvd\")\n",
    "W = nmf.fit_transform(tf_documents)\n",
    "H = nmf.components_\n",
    "\n",
    "for topic_index in range(0, 10):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:10]:\n",
    "        top_terms.append(tf_feature_names[term_index])\n",
    "    print(\"topic %d:\" % (topic_index), top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "intense-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['united', 's', 'states', 'school', 'state', 'president', 'university', 'government', 'college', 'war']\n",
      "topic 1: ['lrb', 'rrb', 'won', 'formula', 'world', 'championship', 'grand', 'race', 'season', 'car']\n",
      "topic 2: ['lrb', 'rrb', 'university', 's', 'used', 'energy', 'called', 'earth', 'known', 'light']\n",
      "topic 3: ['band', 'music', 'album', 'released', 'rock', 'song', 's', 'american', 'singer', 'lrb']\n",
      "topic 4: ['lrb', 'rrb', 'species', 's', 'family', 'long', 'war', '000', 'like', 'years']\n",
      "topic 5: ['france', 'department', 'commune', 'region', 'la', 'saint', 'northern', 'north', 'ã', 'aisne']\n",
      "topic 6: ['rrb', 'lrb', 'born', '1', '2', '4', '3', 'â', 'january', 'american']\n",
      "topic 7: ['s', 'series', 'film', 'lrb', 'rrb', 'book', 'television', 'match', 't', 'written']\n",
      "topic 8: ['lrb', 'rrb', 'district', 'province', 'germany', 'municipality', 'pakistan', 'capital', 'city', 'switzerland']\n",
      "topic 9: ['states', 'united', 'city', 'county', 'game', 's', 'state', 'video', 'florida', 'u']\n",
      "topic 10: ['calais', 'pas', 'north', 'nord', 'europe', 'america', 'asia', 'south', 'africa', 'european']\n",
      "topic 11: ['football', 'lrb', 'rrb', 'team', 'national', 'league', 'player', 'club', 'played', 'plays']\n",
      "topic 12: ['century', 'award', 'used', 'computer', 'awards', 'best', 'los', 'software', 's', 'websites']\n",
      "topic 13: ['s', 'years', 'year', 'later', 'hurricane', 'tropical', 'storm', 'day', 'life', 'million']\n",
      "topic 14: ['king', 'england', 'roman', 'church', 'ii', 'empire', 'town', 'emperor', 'century', 's']\n",
      "topic 15: ['world', 'new', 'york', 'war', 's', 'god', 'city', 'company', 'jesus', 'free']\n",
      "topic 16: ['lrb', 'rrb', 'o', 'language', 'word', 'english', 'wrestling', 'number', 'meaning', 'languages']\n",
      "topic 17: ['used', 'use', 'different', 'line', 'using', 'color', 'people', 'time', 'make', 'usually']\n",
      "topic 18: ['party', 's', 'people', 'references', 'known', 'political', 'rrb', 'lrb', 'minister', 'social']\n",
      "topic 19: ['south', 'located', 'london', 'east', 'north', 'west', 'area', 'river', 'island', 'city']\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20, random_state = 0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_\n",
    "\n",
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "elder-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['rrb', 'lrb', 'd', 'km', 'german', 'e', 'english', 'french', 'c', 'ndash']\n",
      "topic 1: ['s', 'u', 'women', 'state', 'children', 'st', 'death', 'father', 'book', 'king']\n",
      "topic 2: ['france', 'department', 'commune', 'region', 'la', 'calvados', 'normandie', 'basse', 'aisne', 'northern']\n",
      "topic 3: ['united', 'states', 'county', 'kingdom', 'president', 'iowa', 'state', 'canada', 'nations', 'government']\n",
      "topic 4: ['born', 'player', 'football', 'january', 'footballer', 'september', 'july', 'march', 'april', 'plays']\n",
      "topic 5: ['1', '2', '4', 'î', '3', '5', '0', 'ï', '6', '000']\n",
      "topic 6: ['city', 'county', 'capital', 'largest', 'state', 'located', 'population', 'area', 'iowa', 'seat']\n",
      "topic 7: ['pas', 'calais', 'nord', 'region', 'department', 'france', 'commune', 'north', 'l', 'ã']\n",
      "topic 8: ['called', 'people', 'usually', 'group', 'language', 'family', 'area', 'small', 'like', 'english']\n",
      "topic 9: ['football', 'national', 'team', 'league', 'player', 'club', 'played', 'plays', 'hockey', 'japanese']\n",
      "topic 10: ['new', 'york', 'zealand', 'jersey', 'university', 'australia', 'wales', 'state', 'island', 'south']\n",
      "topic 11: ['world', 'war', 'wrestling', 'championship', 'ii', 'wwe', 'cup', 'entertainment', 'professional', 'second']\n",
      "topic 12: ['ð', 'ñ', '4', '3', '2', 'russian', 'serbian', 'cyrillic', 'language', 'à']\n",
      "topic 13: ['â', 'à', 'c', '0', '3', '6', 'f', '10', '5', 'july']\n",
      "topic 14: ['known', 'best', 'better', 'commonly', 'english', 'series', 'species', 'television', 'role', 'simply']\n",
      "topic 15: ['o', 'o2004', 'sã', 'film', 'o2007', 'index', 'o2009', 'o2005', 'o2003', 'o2002']\n",
      "topic 16: ['used', 'term', 'use', 'word', 'english', 'people', 'language', 'usually', 'commonly', 'different']\n",
      "topic 17: ['year', 'time', 'released', 'album', 'game', 'music', 'years', 'people', 'band', 'second']\n",
      "topic 18: ['american', 'film', 'television', 'band', 'singer', 'actor', 'rock', 'series', 'league', 'actress']\n",
      "topic 19: ['north', 'south', 'west', 'east', 'district', 'located', 'river', 'town', 'province', 'america']\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components = 20, random_state = 0, init = \"nndsvd\")\n",
    "W = nmf.fit_transform(tf_documents)\n",
    "H = nmf.components_\n",
    "\n",
    "for topic_index in range(0, 20):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:10]:\n",
    "        top_terms.append(tf_feature_names[term_index])\n",
    "    print(\"topic %d:\" % (topic_index), top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cultural-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.manifold import MDS\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "illegal-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "tfidf_documents = tfidf_vect.fit_transform(df['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "anticipated-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 10, random_state = 0)\n",
    "kmeans.fit(tfidf_documents)\n",
    "\n",
    "#tfidf_documents_normalized = MaxAbsScaler().fit(tfidf_documents).transform(tfidf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "elect-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "guilty-destiny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " city\n",
      " used\n",
      " new\n",
      " people\n",
      " world\n",
      " references\n",
      " called\n",
      " known\n",
      " time\n",
      " 1\n",
      "Cluster 1:\n",
      " football\n",
      " player\n",
      " born\n",
      " rrb\n",
      " lrb\n",
      " japanese\n",
      " team\n",
      " national\n",
      " club\n",
      " plays\n",
      "Cluster 2:\n",
      " district\n",
      " municipality\n",
      " canton\n",
      " switzerland\n",
      " province\n",
      " located\n",
      " belgian\n",
      " aargau\n",
      " pakistan\n",
      " ticino\n",
      "Cluster 3:\n",
      " s\n",
      " u\n",
      " rrb\n",
      " lrb\n",
      " world\n",
      " state\n",
      " city\n",
      " new\n",
      " county\n",
      " time\n",
      "Cluster 4:\n",
      " department\n",
      " france\n",
      " commune\n",
      " region\n",
      " aisne\n",
      " calvados\n",
      " normandie\n",
      " basse\n",
      " picardie\n",
      " gironde\n",
      "Cluster 5:\n",
      " series\n",
      " television\n",
      " game\n",
      " character\n",
      " s\n",
      " animated\n",
      " tv\n",
      " american\n",
      " rrb\n",
      " lrb\n",
      "Cluster 6:\n",
      " rrb\n",
      " lrb\n",
      " born\n",
      " o\n",
      " american\n",
      " known\n",
      " 1\n",
      " d\n",
      " â\n",
      " english\n",
      "Cluster 7:\n",
      " pas\n",
      " calais\n",
      " nord\n",
      " department\n",
      " region\n",
      " france\n",
      " commune\n",
      " north\n",
      " ã\n",
      " l\n",
      "Cluster 8:\n",
      " united\n",
      " states\n",
      " city\n",
      " county\n",
      " iowa\n",
      " kentucky\n",
      " state\n",
      " kingdom\n",
      " illinois\n",
      " florida\n",
      "Cluster 9:\n",
      " released\n",
      " album\n",
      " tropical\n",
      " hurricane\n",
      " storm\n",
      " band\n",
      " single\n",
      " studio\n",
      " atlantic\n",
      " song\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "exempt-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vecs = lda.transform(tf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "coupled-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_20_feature = np.argmax(topic_vecs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "institutional-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lda_20'] = lda_20_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "confirmed-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = \\\n",
    "model_selection.train_test_split( \\\n",
    "df[[\"original_text\", 'dc_proportion', 'AoA_proportion', 'lda_20']],df[\"label\"], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "structural-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "count_vect.fit(train_X['original_text'])\n",
    "xtrain_count =  count_vect.transform(train_X['original_text'])\n",
    "xtest_count =  count_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "tfidf_vect.fit(train_X['original_text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X['original_text'])\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X['original_text'])\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X['original_text'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X['original_text']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X['original_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "compound-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['dc_proportion']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['dc_proportion']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "collected-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['AoA_proportion']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['AoA_proportion']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "continued-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count = scipy.sparse.csr_matrix(hstack([xtrain_count, scipy.sparse.csr_matrix(train_X['lda_20']).T]))\n",
    "xtest_count = scipy.sparse.csr_matrix(hstack([xtest_count, scipy.sparse.csr_matrix(test_X['lda_20']).T]))\n",
    "\n",
    "xtrain_tfidf = scipy.sparse.csr_matrix(hstack([xtrain_tfidf, scipy.sparse.csr_matrix(train_X['lda_20']).T]))\n",
    "xtest_tfidf = scipy.sparse.csr_matrix(hstack([xtest_tfidf, scipy.sparse.csr_matrix(test_X['lda_20']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram, scipy.sparse.csr_matrix(train_X['lda_20']).T]))\n",
    "xtest_tfidf_ngram = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram, scipy.sparse.csr_matrix(test_X['lda_20']).T]))\n",
    "\n",
    "xtrain_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtrain_tfidf_ngram_chars, scipy.sparse.csr_matrix(train_X['lda_20']).T]))\n",
    "xtest_tfidf_ngram_chars = scipy.sparse.csr_matrix(hstack([xtest_tfidf_ngram_chars, scipy.sparse.csr_matrix(test_X['lda_20']).T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "sharp-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6207386363636364\n",
      "0.6468058968058968\n",
      "0.6030213452088452\n",
      "0.6040770884520884\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "functional-kingston",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6980286394348895\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187020116707616\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7061002764127764\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "suffering-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6542824785012284\n",
      "0.6651854269041769\n",
      "0.6578912008599509\n",
      "0.6860699477886978\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "christian-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "tf_documents = count_vect.fit_transform(df['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "worst-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_20 = LatentDirichletAllocation(n_components = 20, random_state = 0)\n",
    "topic_vecs = lda_20.fit_transform(tf_documents)\n",
    "lda_20_feature = np.argmax(topic_vecs, axis = 1)\n",
    "df['lda_20'] = lda_20_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "afraid-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_10 = LatentDirichletAllocation(n_components = 10, random_state = 0)\n",
    "topic_vecs = lda_10.fit_transform(tf_documents)\n",
    "lda_10_feature = np.argmax(topic_vecs, axis = 1)\n",
    "df['lda_10'] = lda_10_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "naughty-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_10 = NMF(n_components = 10, random_state = 0, init = \"nndsvd\")\n",
    "W_10 = nmf_10.fit_transform(tf_documents)\n",
    "H_10 = nmf_10.components_\n",
    "nmf_10_feature = np.argmax(W_10, axis = 1)\n",
    "df['nmf_10'] = lda_10_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "buried-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "nmf_20 = NMF(n_components = 20, random_state = 0, init = \"nndsvd\")\n",
    "W_20 = nmf_20.fit_transform(tf_documents)\n",
    "H_20 = nmf_20.components_\n",
    "nmf_20_feature = np.argmax(W_20, axis = 1)\n",
    "df['nmf_20'] = lda_20_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "noted-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\")\n",
    "tfidf_documents = tfidf_vect.fit_transform(df['original_text'])\n",
    "kmeans_10 = KMeans(n_clusters = 10, random_state = 0)\n",
    "kmeans_10.fit(tfidf_documents)\n",
    "kmeans_10_feature = kmeans_10.labels_\n",
    "df['kmeans_10'] = kmeans_10_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "advisory-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_20 = KMeans(n_clusters = 20, random_state = 0)\n",
    "kmeans_20.fit(tfidf_documents)\n",
    "kmeans_20_feature = kmeans_20.labels_\n",
    "df['kmeans_20'] = kmeans_20_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "extra-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "count_vect.fit(train_X['original_text'])\n",
    "xtrain_count =  count_vect.transform(train_X['original_text'])\n",
    "xtest_count =  count_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*')\n",
    "tfidf_vect.fit(train_X['original_text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_X['original_text'])\n",
    "xtest_tfidf =  tfidf_vect.transform(test_X['original_text'])\n",
    "\n",
    "#tfidf n-gram\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S*', \\\n",
    "                                   ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X['original_text'])\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X['original_text'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\S*', \\\n",
    "                                         ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_X['original_text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X['original_text']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X['original_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "handed-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = \\\n",
    "model_selection.train_test_split( \\\n",
    "df[[\"original_text\", 'dc_proportion', 'AoA_proportion', 'lda_10', 'lda_20', 'nmf_10', 'nmf_20', \\\n",
    "    'kmeans_10', 'kmeans_20']],df[\"label\"], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "valid-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_to_matrix(train_matrix, test_matrix, feature):\n",
    "    train_matrix = scipy.sparse.csr_matrix(hstack([train_matrix, scipy.sparse.csr_matrix(train_X[feature]).T]))\n",
    "    test_matrix = scipy.sparse.csr_matrix(hstack([test_matrix, scipy.sparse.csr_matrix(test_X[feature]).T]))\n",
    "\n",
    "matrix_pairs = [(xtrain_count, xtest_count),(xtrain_tfidf, xtest_tfidf), \\\n",
    "               (xtrain_tfidf_ngram, xtest_tfidf_ngram), (xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars)]     \n",
    "\n",
    "feature_list = ['dc_proportion', 'AoA_proportion', 'lda_10', 'lda_20', 'nmf_10', 'nmf_20', 'kmeans_10', 'kmeans_20']\n",
    "\n",
    "for matrix_pair in matrix_pairs:\n",
    "    for feature in feature_list:\n",
    "        add_feature_to_matrix(matrix_pair[0], matrix_pair[1], feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "acquired-architect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312576, 155410)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "pursuant-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6096149416461917\n",
      "0.6354710534398035\n",
      "0.6310561271498771\n",
      "0.6089623003685504\n"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes\n",
    "\n",
    "multi_nb = naive_bayes.MultinomialNB()\n",
    "multi_nb.fit(xtrain_count,train_y)\n",
    "prediction_1 = multi_nb.predict(xtest_count)\n",
    "print(metrics.accuracy_score(prediction_1, test_y))\n",
    "\n",
    "multi_nb2 = naive_bayes.MultinomialNB()\n",
    "multi_nb2.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_2 = multi_nb2.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(prediction_2, test_y))\n",
    "\n",
    "multi_nb3 = naive_bayes.MultinomialNB()\n",
    "multi_nb3.fit(xtrain_tfidf_ngram_chars,train_y)\n",
    "prediction_3 = multi_nb3.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(prediction_3, test_y))\n",
    "\n",
    "multi_nb4 = naive_bayes.MultinomialNB()\n",
    "multi_nb4.fit(xtrain_tfidf,train_y)\n",
    "prediction_4 = multi_nb4.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(prediction_4, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "radio-hurricane",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6935465294840295\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.7027986793611793\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.718318105036855\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n",
      "0.706512976044226\n",
      "0.4988770730958231\n",
      "0.4982532248157248\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_count, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_count, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_count, train_y)\n",
    "lr_preds = clf.predict(xtest_count)\n",
    "rand_dev_preds = uniform.predict(xtest_count)\n",
    "mf_dev_preds = most_frequent.predict(xtest_count)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter = 500).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "uniform = DummyClassifier(strategy = 'uniform', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "most_frequent = DummyClassifier(strategy = 'most_frequent', random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "lr_preds = clf.predict(xtest_tfidf_ngram_chars)\n",
    "rand_dev_preds = uniform.predict(xtest_tfidf_ngram_chars)\n",
    "mf_dev_preds = most_frequent.predict(xtest_tfidf_ngram_chars)\n",
    "print(metrics.accuracy_score(lr_preds, test_y))\n",
    "print(metrics.accuracy_score(rand_dev_preds, test_y))\n",
    "print(metrics.accuracy_score(mf_dev_preds, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "imposed-poetry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505681818181818\n",
      "0.657228961916462\n",
      "0.6626132524570024\n",
      "0.6858204084766585\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "\n",
    "rf_clf_count = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_count, train_y)\n",
    "rf_clf_tfidf = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf, train_y)\n",
    "rf_clf_tfidf_ngram = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram, train_y)\n",
    "rf_clf_tfidf_ngram_chars = RandomForestClassifier(n_estimators = 50, max_depth = 15, random_state = 0).fit(xtrain_tfidf_ngram_chars, train_y)\n",
    "\n",
    "rf_count_preds = rf_clf_count.predict(xtest_count)\n",
    "rf_tfidf_preds = rf_clf_tfidf.predict(xtest_tfidf)\n",
    "rf_tfidf_ngram_preds = rf_clf_tfidf_ngram.predict(xtest_tfidf_ngram)\n",
    "rf_tfidf_ngram_chars_preds = rf_clf_tfidf_ngram_chars.predict(xtest_tfidf_ngram_chars)\n",
    "\n",
    "print(metrics.accuracy_score(rf_count_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_preds, test_y))\n",
    "print(metrics.accuracy_score(rf_tfidf_ngram_chars_preds, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "specified-framework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>dc_proportion</th>\n",
       "      <th>AoA_proportion</th>\n",
       "      <th>lda_20</th>\n",
       "      <th>lda_10</th>\n",
       "      <th>nmf_10</th>\n",
       "      <th>nmf_20</th>\n",
       "      <th>kmeans_10</th>\n",
       "      <th>kmeans_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>However , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>If it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>Calgary Stampeders ,</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label  \\\n",
       "0       There is manuscript evidence that Austen conti...      1   \n",
       "1       In a remarkable comparative analysis , Mandaea...      1   \n",
       "2       Before Persephone was released to Hermes , who...      1   \n",
       "3       Cogeneration plants are commonly found in dist...      1   \n",
       "4       Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1   \n",
       "...                                                   ...    ...   \n",
       "416763  A Duke Nukem 3D version has been sold for Xbox...      0   \n",
       "416764  However , it is becoming replaced as a method ...      0   \n",
       "416765  There are hand gestures in both Hindu and Budd...      0   \n",
       "416766  If it is necessary to use colors , try to choo...      0   \n",
       "416767                               Calgary Stampeders ,      0   \n",
       "\n",
       "        dc_proportion  AoA_proportion  lda_20  lda_10  nmf_10  nmf_20  \\\n",
       "0            0.465116        0.651163       7       7       7       7   \n",
       "1            0.217391        0.521739      15       2       2      15   \n",
       "2            0.543478        0.782609      13       7       7      13   \n",
       "3            0.256410        0.692308       2       2       2       2   \n",
       "4            0.388889        0.416667       8       8       8       8   \n",
       "...               ...             ...     ...     ...     ...     ...   \n",
       "416763       0.352941        0.352941      13       9       9      13   \n",
       "416764       0.550000        0.750000       0       0       0       0   \n",
       "416765       0.454545        0.636364       4       2       2       4   \n",
       "416766       0.595238        0.809524      17       2       2      17   \n",
       "416767       0.000000        0.000000      11       5       5      11   \n",
       "\n",
       "        kmeans_10  kmeans_20  \n",
       "0               0          7  \n",
       "1               3         11  \n",
       "2               6         19  \n",
       "3               0          7  \n",
       "4               6         19  \n",
       "...           ...        ...  \n",
       "416763          0          7  \n",
       "416764          8          3  \n",
       "416765          0          7  \n",
       "416766          6         19  \n",
       "416767          0          7  \n",
       "\n",
       "[416768 rows x 10 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "tamil-boutique",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-dad6c1584286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mAoA_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAoA_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mAoA_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAoA_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#df['dc_proportion'] = result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-345-dad6c1584286>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mAoA_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAoA_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mAoA_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAoA_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#df['dc_proportion'] = result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-345-dad6c1584286>\u001b[0m in \u001b[0;36mAoA_sum\u001b[0;34m(AoA_counter, s)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAoA_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mAoA_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAoA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAoA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AoA_Kup_lem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAoA_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4937\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4939\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4941\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def AoA_sum(AoA_counter, s):\n",
    "    split = s.split()\n",
    "    counter = Counter(split)\n",
    "    both = counter & AoA_counter\n",
    "    AoA_sum = 0\n",
    "    for word in both.keys():\n",
    "        AoA_sum += np.float(AoA.loc[AoA.Word == word]['AoA_Kup_lem'])\n",
    "    return AoA_sum\n",
    "\n",
    "AoA_counter = Counter(AoA_vocab)\n",
    "result = [AoA_sum(AoA_counter, s) for s in df['original_text']]\n",
    "#df['dc_proportion'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "trained-fundamental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128.29000000000002]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "superb-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>dc_proportion</th>\n",
       "      <th>AoA_proportion</th>\n",
       "      <th>lda_20</th>\n",
       "      <th>lda_10</th>\n",
       "      <th>nmf_10</th>\n",
       "      <th>nmf_20</th>\n",
       "      <th>kmeans_10</th>\n",
       "      <th>kmeans_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  label  dc_proportion  \\\n",
       "0  There is manuscript evidence that Austen conti...      1       0.465116   \n",
       "\n",
       "   AoA_proportion  lda_20  lda_10  nmf_10  nmf_20  kmeans_10  kmeans_20  \n",
       "0        0.651163       7       7       7       7          0          7  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "lightweight-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  Nsyll  \\\n",
       "0    a                    a  20415.27         Article         1      1      1   \n",
       "\n",
       "  Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0                 a     2.89         1.0         2.89             1.0   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA.loc[AoA.Word == 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-legislation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
